import matplotlib.pyplot as plt
import numpy as np
from sklearn.cross_validation import train_test_split
import helpers
import regularized_gradient_descent
import math
import batch_gradient_descent

numExamples=150
numFeatures=75
csvFileName = "hw2_autogen_data.csv"

#1.1 Dataset construction
def generateData():
    samples = np.random.rand(numExamples, numFeatures)
    trueWeight = np.zeros(numFeatures)
    for index in range(10):
        trueWeight[index] = 10
    for index in range(10,20):
        trueWeight[index] = -10
    
    noise = np.random.randn(numExamples)
    trueOutput = np.dot(samples,trueWeight) + noise
    
    dataToStore = np.c_[samples, trueOutput];
    
    np.savetxt(csvFileName, dataToStore,delimiter=",")

def splitData(dataArray):
    X = dataArray[:,:-1]
    y = dataArray[:,-1]
    
    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=80, random_state=10)
    X_test, X_validate, y_test, y_validate = train_test_split(X_test, y_test, train_size=50, random_state=10)
    
    return helpers.data(X_train, X_validate, X_test, y_train, y_validate, y_test)

def doGridSearch(data, params):
    log_lambdas=np.arange(-5, 2, 0.5)
    lambdas = np.power(10, log_lambdas)
    
    #get best lambda
    best_loss = np.Inf
    best_lambda = 0
    best_theta = np.zeros(data.train_x.shape[1])
    for lambda_reg in lambdas:
        params.lambda_reg = lambda_reg
        theta_hist, loss_hist = regularized_gradient_descent.run(data.train_x, data.train_y, params)
        loss = batch_gradient_descent.compute_loss(data.validation_x, data.validation_y, theta_hist[-1,:])
        print "lambda: " +str(lambda_reg) +", loss: " +str(loss)
        if(loss<best_loss):
            best_loss = loss
            best_lambda = params.lambda_reg
            best_theta = theta_hist[-1,:]
    print best_lambda
    #print best_theta
    
        
# reading the data from csv
def main():
    #generateData()
    dataArray = np.loadtxt(csvFileName, delimiter=",")
    
    data = splitData(dataArray)
    print "range of y: " +str(data.train_y.max()-data.train_y.min())    
    
    reg_params = regularized_gradient_descent.params(initial_bias=12, alpha=0.01, num_iter=1000, use_grad_checker=False)
    data.train_x = helpers.add_bias_term(data.train_x, reg_params.initial_bias)
    data.test_x = helpers.add_bias_term(data.test_x, reg_params.initial_bias)
    data.validation_x = helpers.add_bias_term(data.validation_x, reg_params.initial_bias)
    doGridSearch(data, reg_params)
    
if __name__ == "__main__":
    main()
